---
title: "ST558 Project 2"
author: "David Weck"
date: "6/20/2020"
output:
    rmarkdown::github_document:
      toc: true
      toc_depth: 1
params:
  day: weekday_is_monday
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

#Loading requires packages
library(tidyverse)
library(rmarkdown)
library(RColorBrewer)
library(caret)

```
  
# Introduction  
  
The dataset I will be using for this project comes from the UC Irvine Machine Learning Repository. It can be found [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity). It contains information about articles posted by Mashable over a 2 year period. The target variable in the dataset is the number of shares. There are 58 features in this dataset. These features include items like number of words in the title, number of images in the article, polarity scores, etc. The objective of my analysis is to use some of these features to predict the number of shares that an article will receive. To do this, I will be fitting one linear regression model using the lasso and one non-linear ensemble model. I will be automating R Markdown to create a separate analysis for each day of the week - i.e there will be 7 different analyses: one for articles posted on Monday, one for articles posted on Tuesday, etc.  
  
# Data  
  
This section loads the data, filters to selected day of the week, removes irrelevant columns, and creates a training and a test set.
  
```{r data}

#Loading Data
news <- read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')

#Filtering to include only data for selected day of the week
#Removing non-predictive columns, columns associated with weekday, and columns about LDA closeness
day_data <- news %>%
  filter(get(params$day) == 1) %>%
  select(3:31, 45:61)

#Creating training and testing set
#Setting seed for reproducibility
set.seed(68)

train_index <- sample(1:nrow(day_data), size = .7 * nrow(day_data))

train <- day_data[train_index, ]
train_x <- train[ , -46]
train_y <- train[ , 46]

test <- day_data[-train_index, ]
test_x <- test[ , -46]
test_y <- test[ , 46]

```
  
In the data section above, I removed columns 1, 2, and 32-44. Columns 1 and 2 contained the URL to the post and the number of days between when the article was published and this dataset was generated. Both of these columns are non predictive. Columns 32-39 contain indicator variables for the day of the week that the article was published. Because we are already filtering so the dataset only contains posts for a selected day of the week, these columns become irrelevant. Finally, columns 40-44 contain LDA closeness scores for different topics. These topics are unknown so these columns were removed for the sake of clarity.  
  
After removing these columns, we are left with 45 features. 11 of these features are related to words/content in the article title and/or the article itself. The include features such as number of words in the title/article, average word length, number of images in the article, number of videos in the article, etc. 6 of these features are indicators of the data channel of the article, i.e lifestyle, entertainment, business, social media, tech, or world. 12 columns contain information on the min, max, and average number of shares of keywords in the article and the number of shares of other Mashable articles referenced in the article. Finally, 16 columns contain information about the sentiment of the article. Some of these columns include the rate of positive/negative words, the text subjectivity, the title sentiment polarity, etc.  
  
# EDA  
  
This section performs some exploratory data analysis to get an overview of the data we are working with.  
  
```{r target}

#Getting summary stats of the response
summary(train_y)

#Plotting a histogram of the response
g <- ggplot(train, aes(x = shares))
g + geom_histogram(fill = 'skyblue')

#Response is heavily skewed, plotting histogram after log transform
g <- ggplot(train, aes(x = log(shares)))
g + geom_histogram(fill = 'skyblue')

```
  
The first plot above shows a histogram of the target variable, shares. We can see the number of shares is heavily skewed. The histogram of log shares looks better.  I will use log shares as the response.  
  
```{r boxplot}

#Making a boxplot of of log(shares)
h <- ggplot(train, aes(x = params$day, y = log(shares)))
h + geom_boxplot(fill = 'skyblue') +
  theme(axis.text.x = element_blank()) +
  labs(x = str_to_title(str_sub(params$day, start = 12)), y = 'Log Shares',
       title = paste('Log Shares on', str_to_title(str_sub(params$day, start = 12))))
```
  
Since there are outliers present, I will create a training set with outliers removed. I will fit models to the original training set and the training set with outliers removed.   
  
```{r barchart}

#Creating a summary table of average shares per each data channel
temp <- train %>%
  group_by(data_channel_is_bus, data_channel_is_entertainment,  #Grouping by data channel columns
           data_channel_is_lifestyle, data_channel_is_socmed, 
           data_channel_is_tech, data_channel_is_world) %>%
  summarize(med_shares = median(shares)) %>%                      #Creating summary stat
  gather(1:6, key = 'channel', value = 'indicator') %>%         #Creating categorical Channel column
  filter(indicator == 1)                                        #Filtering to include necessary rows

#Creating a bar chart of median shares per channel
j <- ggplot(temp, aes(x = channel, y = med_shares))
j + geom_bar(aes(fill = channel), stat = 'identity', show.legend = FALSE) +
  scale_x_discrete(labels = c('Business', 'Entertainment', 'Lifestyle', 
                              'Social Media', 'Technology', 'World' )) +
  scale_fill_brewer(palette = 'Set2') +
  labs(title = 'Median Shares by Data Channel', x = 'Channel', y = 'Median Shares')

```
  
The plot above shows of the median shares per channel for `r str_to_title(str_sub(params$day, start = 12))`.  
  
# Modelling  
  
# Automation  
  
